{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7876147,"sourceType":"datasetVersion","datasetId":4622139},{"sourceId":7918368,"sourceType":"datasetVersion","datasetId":4652900}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Text Pre-processing**","metadata":{}},{"cell_type":"code","source":"file=open(\"/kaggle/input/textfile/words_250000_train.txt\",\"r\")\nwords=list(map(lambda x: x[:-1],file.readlines()))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=[]\n\nfor word in words:\n    \n    text+=list(set(list(word)))\n\ntext=set(text)\ntext=list(sorted(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizerMap={text[i] : i for i in range(len(text))}\ndetokenizerMap={i : text[i] for i in range(len(text))}\ntokenizerMap[\"_\"]=29\ntokenizerMap[\"<start>\"]=-10\ntokenizerMap[\"<end>\"]=-20\n\ndetokenizerMap[29]=\"_\"\ndetokenizerMap[-10]=\"<start>\"\ndetokenizerMap[-20]=\"<end>\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenizer(word):\n    \n    array=[]\n    \n    for i in word:\n        \n        array.append(tokenizerMap[i])\n\n    return array\n\ndef detokenizer(array):\n    \n    string=\"\"\n    \n    for i in array:\n        \n        string+=detokenizerMap[i]\n    \n    return string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"string: \",detokenizer([17, 29, 6, 29, 0,]))\nprint(\"token:  \",tokenizer(\"r_g_a\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Generation**","metadata":{}},{"cell_type":"code","source":"from random import shuffle\nfrom functools import wraps\nfrom tqdm import tqdm \nimport joblib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ir=0\ninputData=[]\noutputData=[]\ndataSet={}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfor word in tqdm(words):\n\n    n=len(word)\n\n    tempInput=[]\n    tempOutput=[]\n    rejected=[]\n    rejectedSequence=list(set(text)-set(word))\n    rejectedSequence=tuple(sorted(set(rejectedSequence)))\n    n2=len(rejectedSequence)\n\n    cache={}\n\n    def char_occurrences(string):\n\n        occurrences = {}\n        for index,char in enumerate(string):\n            if char in occurrences:\n                occurrences[char].append(index)\n            else:\n                occurrences[char] = [string.index(char)]\n        return occurrences\n\n\n    def index_char(string):\n\n        Map={i:string[i] for i in range(len(string))}\n        return Map\n\n    input_string=word\n    result = char_occurrences(input_string)\n    ans=index_char(input_string)\n\n\n    keys=list(result.keys())\n    n=len(keys)\n\n    def subsequence(index,array):\n\n        if index==n:\n            tempInput.append([array])\n            op=\"_\"\n\n            for index,i in enumerate(array):\n\n                if i==\"_\":\n                    op=ans[index]\n                    break\n\n            tempOutput.append([op])\n            return\n\n        b=array.copy()\n\n        for i in result[keys[index]]:\n\n            b[i]=keys[index]\n\n        subsequence(index+1,b)\n        subsequence(index+1,array)\n\n    subsequence(0,[\"_\" for i in range(len(input_string))])\n\n\n\n\n\n    dataSet[ir]={\n\n        \"inputs\":tempInput,\n        \"outputs\":tempOutput,\n\n    }\n\n    ir+=1\n\"\"\"\n\n\"\"\"joblib.dump(dataSet,f\"/kaggle/working/dataset{ir}.joblib\")\ndel dataSet\ndataSet={}\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/hangman-csv/data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[1][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trueData=[]\n\nfor i in tqdm(range(1,227300%53)):\n\n    exec(\"ip=\"+data.loc[0][i])\n    exec(\"op=\"+data.loc[1][i])\n\n    for j in tqdm(range(len(ip))):\n\n        trueData+=[[ip[j][0],op[j]]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(trueData)\nrandom.shuffle(trueData)\nrandom.shuffle(trueData)\nrandom.shuffle(trueData)\nrandom.shuffle(trueData)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trueData=trueData[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trueData)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras_nlp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer, Dense, Embedding, PReLU, Input, Add, LayerNormalization,Dropout\nfrom keras_nlp.layers import TokenAndPositionEmbedding,StartEndPacker\nfrom tensorflow.keras.losses import cosine_similarity,binary_crossentropy,SparseCategoricalCrossentropy\nfrom tensorflow import GradientTape\nfrom keras.activations import relu,softmax,sigmoid, selu\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import Model\nimport tensorflow as tf\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(Layer):\n    \n    def __init__(self,embedding_size):\n        super().__init__()\n        \n        self.key=Dense(embedding_size,activation=\"linear\")\n        self.query=Dense(embedding_size,activation=\"linear\")\n        self.value=Dense(embedding_size,activation=\"linear\")\n        self.ln=LayerNormalization()\n        self.pReluAct=PReLU()\n        self.dropout=Dropout(0.1)\n    \n    def call(self,inputs):\n        \n        key=self.key(inputs)\n        query=self.query(inputs)\n        value=self.value(inputs)\n        \n        key=tf.matmul(query,key,transpose_b=True)\n        key=self.pReluAct(key)\n        \n        value=tf.matmul(key,value)\n        \n        value=self.ln(value)\n        value=self.dropout(value)\n        value=softmax(value)\n        \n        return value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(Layer):\n    \n    def __init__(self,num_heads,embedding_size):\n        super().__init__()\n        \n        self.num_heads=num_heads\n        self.embedding_size=embedding_size\n        self.head_size=embedding_size//num_heads\n    \n    def build(self,inputShape):\n        \n        self.mha=[SelfAttention(self.head_size) for i in range(self.num_heads)]\n\n    def split_heads(self, x):\n        \n        x = tf.reshape(x, (-1, self.num_heads, self.head_size))\n        return x\n    \n    def call(self,inputs):\n        \n        \n        patches=self.split_heads(inputs)\n        \n        c=[]\n        \n        for mh in range(self.num_heads):\n            \n            c.append(self.mha[mh](patches[mh]))\n        \n        cLayer=tf.concat(c,axis=-1)\n        \n        return cLayer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Node(Layer):\n    \n    def __init__(self,vocabSize,embedSize,numHeads):\n        super().__init__()\n        \n        self.vocabSize=vocabSize\n        self.embedSize=embedSize\n        self.numHeads=numHeads\n        \n        self.selfAttention=SelfAttention(self.embedSize)\n        self.mha=MultiHeadAttention(self.numHeads,self.embedSize)\n    \n    def call(self,inputs):\n        \n        selfAttention=self.selfAttention(inputs)\n        mha=self.mha(selfAttention)\n        \n        return mha","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from random import randint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NGramModel(Model):\n    \n    def __init__(self,vocabSize,embedSize,numHeads):\n        super().__init__()\n        \n        self.optimizer=Adam(0.01)\n        self.vocabSize=vocabSize\n        self.embedSize=embedSize\n        self.numHeads=numHeads\n        \n        self.packer=StartEndPacker(vocabSize)\n        self.embedding=TokenAndPositionEmbedding(vocabSize,vocabSize,embedSize)\n        \n        self.startNodes=[Node(vocabSize,embedSize,numHeads) for i in range(4)]\n        \n        self.endNodes=[Node(vocabSize,30,numHeads) for i in range(5)]\n        \n        self.dense2=[Dense(1000,activation=\"relu\",) for i in range(4)]\n        \n        self.dense=[Dense(30*i,activation=\"linear\") for i in range(6,0,-1)]\n        \n        self.d=Dense(30)\n        \n        self.addition=Add()\n        \n        self.loss=SparseCategoricalCrossentropy(\n            from_logits=True,\n        )\n\n        self.cache=[]\n        self.iterator=0\n        self.epoch=1\n    \n    def call(self,inputs,rejectedTargets):\n        \n        packed=self.packer(inputs)\n        logits=self.embedding(packed)\n        \n        for layer in self.startNodes:\n            lg=logits\n            logits=layer(logits)\n            logits=self.addition([selu(logits),lg])\n        \n        \n        logits=self.d(logits)\n        \n        mask=np.ones((30,30))\n        if rejectedTargets!=[]:\n            mask[:,rejectedTargets]=0\n    \n        \n        for layer in self.endNodes:\n            lg=logits\n            logits=tf.multiply(logits,mask)\n\n            logits=layer(logits)\n            logits=self.addition([selu(logits),lg])\n        \n        logits=tf.multiply(logits,mask)\n        logits=softmax(logits)\n        \n        for layer in self.dense2:\n            \n            logits=layer(logits)\n        \n        for layer in self.dense:\n            \n            logits=layer(logits)\n\n        return logits\n    \n    def processors(self,tokenizer,detokenizer):\n        \n        self.tokenizer=tokenizer\n        self.detokenizer=detokenizer\n\n    def cLoss(self,logits=None,trueTargets=None):\n        \n        loss=self.loss(trueTargets,logits)\n        return loss\n    \n    def loadData(self,data):\n        \n        self.data=data\n    \n    def sampleData(self):\n        \n        inputs=self.tokenizer(trueData[self.iterator][0])\n        outputs=self.tokenizer(trueData[self.iterator][1])\n        \n        self.iterator=(self.iterator+1)%50\n        \n        \n        \n        return inputs,outputs,[]\n        \n    def isRejected(self,idx,op):\n        \n        if idx!=op:\n            return True\n        \n        else:\n            return False\n    \n    def cacheData(self,inputs,outputs,rejectedSeq,idx):\n        \n        if len(rejectedSeq)>=2:\n            return\n        \n        if self.isRejected(idx,outputs):\n            \n            self.cache.append([inputs,outputs,rejectedSeq+[idx]])\n    \n    def calcIdx(self,logits):\n        \n        vector=logits[-1,:]\n        idx=tf.math.argmax(vector)\n        \n        return idx\n    \n    def processOp(self,logits,inputs,outputs,rejectedSeq):\n        \n        idx=self.calcIdx(logits)\n        self.cacheData(inputs,outputs,rejectedSeq,idx)\n    \n    def lossfn(self,logits,trueTargets):\n        \n        vector=logits[-1,:]\n        loss=self.cLoss(vector,tf.constant(trueTargets))\n        \n        return loss\n    \n    def sampleCache(self):\n        \n        if self.cache!=[]:\n        \n            element=self.cache.pop(0)\n        \n        else:\n            \n            return False\n        \n        return element\n    \n    def train(self,steps):\n        \n        for step in range(steps):\n            \n            if self.iterator==0 or self.iterator==1:\n                self.epoch+=1\n        \n            for _ in range(2):\n\n                with GradientTape() as tape:\n\n                    inputs,trueTargets,rejectedSeq=self.sampleData()\n                    logits=self.call(inputs,rejectedSeq)\n                    loss=self.lossfn(logits,tf.constant(trueTargets))\n\n                self.processOp(logits,inputs,trueTargets,[])\n\n                gradients=tape.gradient(loss,self.trainable_variables)\n                self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n                \n                print(f\"True Step epoch {self.epoch} iter {self.iterator}: \",float(loss))\n                \n                \n            for _ in range(3):\n\n                if self.cache!=[]:\n\n                    with GradientTape() as tape:\n\n                        inputs,trueTargets,rejectedSeq=self.sampleCache()\n                        logits=self.call(inputs,rejectedSeq)\n                        loss=self.lossfn(logits,tf.constant(trueTargets))\n\n                    self.processOp(logits,inputs,trueTargets,rejectedSeq)\n\n                    gradients=tape.gradient(loss,self.trainable_variables)\n                    self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n                    \n                    print(f\"Cache Step epoch {self.epoch} iter {self.iterator}: \",float(loss))\n                \n            if self.iterator>=0 and self.iterator<=1:\n                \n                while self.cache!=[]:\n                    \n                    with GradientTape() as tape:\n\n                        inputs,trueTargets,rejectedSeq=self.sampleCache()\n                        logits=self.call(inputs,rejectedSeq)\n                        loss=self.lossfn(logits,tf.constant(trueTargets))\n\n                    self.processOp(logits,inputs,trueTargets,rejectedSeq)\n\n                    gradients=tape.gradient(loss,self.trainable_variables)\n                    self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n                    \n                    print(f\"Cache Step epoch {self.epoch} iter {self.iterator}: \",float(loss))\n                    \n                \n                \n        \n                else:\n                    break\n    \n    def generate(self):\n        pass    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=NGramModel(30,3000,30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.loadData(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.processors(tokenizer,detokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train(100000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}